{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "rCOSt7J8gkwS",
      "metadata": {
        "id": "rCOSt7J8gkwS"
      },
      "source": [
        "#ATUHAIRE PAULINE\n",
        "#ASSIGNMENT 2 DATA LIFE CYCLE\n",
        "#ACCESS NUMBER:B5093"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db271631",
      "metadata": {},
      "source": [
        "### NEW CHANGES"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af23ec2",
      "metadata": {
        "id": "4af23ec2"
      },
      "source": [
        "#CASSAVA YIELD ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3920f8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "e3920f8e",
        "outputId": "1f6219e7-a22e-462b-ad2f-6dd48491bef3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel 'base (Python 3.13.5)' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# --- Cell 1: Imports & Load workbook ---\n",
        "import os\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, warnings\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.rcParams['figure.figsize'] = (6,3.5)\n",
        "\n",
        "# Load file (path used by the assistant environment)\n",
        "os.chdir(r'C:\\Users\\AA\\Desktop\\DLC ASSIGNMENT2')\n",
        "file_path = \"Cassava_Yield_Data.xlsx\"\n",
        "# print(\"Loading:\", file_path)\n",
        "df = pd.read_excel(file_path)\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Normalize column names: strip spaces and lowercase a helper map for known variants\n",
        "df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n",
        "# map likely variants to standard names\n",
        "col_map = {}\n",
        "for c in df.columns:\n",
        "    lc = c.lower().replace(' ', '')\n",
        "    if lc in ['fertilizer','fertiliser','fertilizers','fert','fertiliserinfo','fertilizersused']:\n",
        "        col_map[c] = 'fertilizer'\n",
        "    if lc in ['tillage','tillage_method','tillage_'] or c.lower().strip().startswith('tillage'):\n",
        "        col_map[c] = 'tillage'\n",
        "    if lc in ['sesn','season']:\n",
        "        col_map[c] = 'Sesn'\n",
        "    if lc in ['totalweightperhectare','totalweightperhectare (kg/ha)'.lower().replace(' ','')]:\n",
        "        col_map[c] = 'TotalWeightperhectare'\n",
        "    if lc in ['totaltuberperhectare','totaltuberperhectare (kg/ha)'.lower().replace(' ','')]:\n",
        "        col_map[c] = 'TotalTuberperHectare'\n",
        "    if lc in ['plants_harvested','plantsharvested','plants_harvested']:\n",
        "        col_map[c] = 'Plants_harvested'\n",
        "\n",
        "\n",
        "for orig in list(df.columns):\n",
        "    if orig.strip().lower() == 'fert' or orig.strip().lower()=='fertt' or orig.strip().lower()=='fertt':\n",
        "        col_map[orig] = 'fertilizer'\n",
        "    if orig.strip().lower() == 'fert' :\n",
        "        col_map[orig] = 'fertilizer'\n",
        "    if orig.strip() == 'ferT':\n",
        "        col_map[orig] = 'fertilizer'\n",
        "    if orig.strip() == 'tillage ':\n",
        "        col_map[orig] = 'tillage'\n",
        "\n",
        "# apply renaming\n",
        "if col_map:\n",
        "    print(\"Applying column renames (sample):\", list(col_map.items())[:10])\n",
        "    df = df.rename(columns=col_map)\n",
        "\n",
        "# Show the final columns after normalization\n",
        "print(\"Normalized columns:\")\n",
        "print(df.columns.tolist())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73455956",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "73455956",
        "outputId": "e0e4f895-9677-4fbe-ade4-db507be22d12"
      },
      "outputs": [],
      "source": [
        "#  Q1: DATA EXPLORATION, MISSING DATA, OUTLIERS, AND CLEANING\n",
        "\n",
        "# Select columns range for analysis\n",
        "\n",
        "cols = list(df.columns)\n",
        "if 'Sesn' in cols and 'TotalTuberperHectare' in cols:\n",
        "    start, end = cols.index('Sesn'), cols.index('TotalTuberperHectare') + 1\n",
        "    eda_cols = cols[start:end]\n",
        "else:\n",
        "    eda_cols = cols  # fallback\n",
        "eda_df = df[eda_cols].copy()\n",
        "\n",
        "print(\" Columns selected for EDA ({}):\".format(len(eda_cols)))\n",
        "print(eda_cols)\n",
        "\n",
        "#BASIC STRUCTURE \n",
        "display(eda_df.info())\n",
        "display(eda_df.describe(include='all').T)\n",
        "\n",
        "# MISSING VALUES\n",
        "missing = eda_df.isnull().sum().sort_values(ascending=False)\n",
        "print(\"\\n Missing values per column:\")\n",
        "if missing.sum() == 0:\n",
        "    print(\"No missing values detected.\")\n",
        "else:\n",
        "    display(missing[missing > 0])\n",
        "\n",
        "#NUMERIC VARIABLES AND DISTRIBUTIONS \n",
        "numeric_cols = eda_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(\"\\n Numeric columns:\", numeric_cols)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.histplot(eda_df[col].dropna(), bins=30, kde=True, color='steelblue')\n",
        "    plt.title(f\"Distribution of {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# OUTLIER DETECTION (IQR METHOD)\n",
        "outlier_counts = {}\n",
        "for col in numeric_cols:\n",
        "    s = eda_df[col].dropna()\n",
        "    Q1, Q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    outlier_counts[col] = int(((s < lower) | (s > upper)).sum())\n",
        "\n",
        "outlier_counts = pd.Series(outlier_counts).sort_values(ascending=False)\n",
        "print(\"\\n Outlier counts detected (using IQR):\")\n",
        "display(outlier_counts[outlier_counts > 0])\n",
        "\n",
        "#DATA TRANSFORMATION: HANDLE MISSING + OUTLIERS\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Impute missing numeric values with median\n",
        "if numeric_cols:\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    df_clean[numeric_cols] = imputer.fit_transform(df_clean[numeric_cols])\n",
        "\n",
        "# Cap outliers at 1st and 99th percentiles\n",
        "for col in numeric_cols:\n",
        "    low, high = df_clean[col].quantile(0.01), df_clean[col].quantile(0.99)\n",
        "    df_clean[col] = df_clean[col].clip(lower=low, upper=high)\n",
        "\n",
        "print(\"\\n Data Transformation Complete:\")\n",
        "print(\"→ Missing numeric values replaced with median\")\n",
        "print(\"→ Outliers capped at 1st and 99th percentiles\")\n",
        "\n",
        "# POST-CLEANING SUMMARY\n",
        "display(df_clean[numeric_cols].describe().T)\n",
        "\n",
        "# Save clean DataFrame\n",
        "print(\"\\n Clean dataset (df_clean) ready for later analysis.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd96256",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bbd96256",
        "outputId": "5e1ca1db-2527-49a2-f000-9b4f9439d66e"
      },
      "outputs": [],
      "source": [
        "# Q2: Investigate relationships between variables using displays and statistical tests\n",
        "# a) Two continuous variables\n",
        "# b) One continuous variable and one categorical variable\n",
        "# c) Two categorical variables\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "# Define variable names (edit if needed)\n",
        "cont1 = 'TotalWeightperhectare'    # continuous 1\n",
        "cont2 = 'TotalTuberperHectare'     # continuous 2\n",
        "cat = 'fertilizer'                 # categorical (for cont vs cat)\n",
        "catA = 'fertilizer'                # categorical A (for cat vs cat)\n",
        "catB = 'Sesn'                      # categorical B (for cat vs cat)\n",
        "\n",
        "print(\"Available columns:\", df_clean.columns.tolist())\n",
        "\n",
        "# (a) Continuous vs Continuous\n",
        "if cont1 in df_clean.columns and cont2 in df_clean.columns:\n",
        "    x = df_clean[cont1]\n",
        "    y = df_clean[cont2]\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.regplot(x=x, y=y, scatter_kws={'alpha':0.7})\n",
        "    plt.title(f\"Relationship between {cont1} and {cont2}\")\n",
        "    plt.xlabel(cont1)\n",
        "    plt.ylabel(cont2)\n",
        "    plt.show()\n",
        "\n",
        "    pearson_r, pearson_p = stats.pearsonr(x, y)\n",
        "    spearman_r, spearman_p = stats.spearmanr(x, y)\n",
        "\n",
        "    print(f\" Pearson correlation: r={pearson_r:.4f}, p={pearson_p:.4g}\")\n",
        "    print(f\" Spearman correlation: rho={spearman_r:.4f}, p={spearman_p:.4g}\")\n",
        "\n",
        "else:\n",
        "    print(\" One or both continuous variables not found.\")\n",
        "\n",
        "# (b) Continuous vs Categorical\n",
        "\n",
        "if cont1 in df_clean.columns and cat in df_clean.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.boxplot(x=cat, y=cont1, data=df_clean)\n",
        "    plt.title(f\"Distribution of {cont1} by {cat}\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    groups = [g[cont1].values for n,g in df_clean.groupby(cat)]\n",
        "    if len(groups) > 1:\n",
        "        try:\n",
        "            F, p_anova = stats.f_oneway(*groups)\n",
        "            print(f\"ANOVA test: F={F:.3f}, p={p_anova:.5f}\")\n",
        "        except Exception as e:\n",
        "            print(\"ANOVA error:\", e)\n",
        "\n",
        "        try:\n",
        "            H, p_kruskal = stats.kruskal(*groups)\n",
        "            print(f\"Kruskal–Wallis test: H={H:.3f}, p={p_kruskal:.5f}\")\n",
        "        except Exception as e:\n",
        "            print(\"Kruskal error:\", e)\n",
        "else:\n",
        "    print(\" Variables for part (b) not found.\")\n",
        "\n",
        "# (c) Categorical vs Categorical\n",
        "\n",
        "if catA in df_clean.columns and catB in df_clean.columns:\n",
        "    ct = pd.crosstab(df_clean[catA], df_clean[catB])\n",
        "    print(\"\\nContingency Table:\")\n",
        "    display(ct)\n",
        "\n",
        "    chi2, p_chi, dof, expected = stats.chi2_contingency(ct.fillna(0))\n",
        "    print(f\"Chi-square test: χ²={chi2:.4f}, p={p_chi:.4g}, dof={dof}\")\n",
        "else:\n",
        "    print(\" Categorical variables for part (c) not found.\")\n",
        "\n",
        "\n",
        "#  INTERPRETATION SUMMARY\n",
        "print(\"\\n INTERPRETATION SUMMARY\")\n",
        "\n",
        "# (a) Continuous–Continuous\n",
        "if pearson_p < 0.05 or spearman_p < 0.05:\n",
        "    print(f\"\\n→ (a) There is a significant positive correlation between {cont1} and {cont2} \"\n",
        "          f\"(Pearson r={pearson_r:.3f}, p={pearson_p:.3g}).\")\n",
        "    print(\"   This suggests that as the total tuber weight increases, the number of tubers per hectare also tends to increase.\")\n",
        "else:\n",
        "    print(f\"\\n→ (a) No significant correlation detected between {cont1} and {cont2} (p > 0.05).\")\n",
        "\n",
        "# (b) Continuous–Categorical\n",
        "if 'p_kruskal' in locals() and p_kruskal < 0.05:\n",
        "    print(f\"\\n→ (b) There is a statistically significant difference in {cont1} across fertilizer types (Kruskal p={p_kruskal:.3g}).\")\n",
        "    print(\"   Some fertilizers appear to produce higher yields than others.\")\n",
        "elif 'p_anova' in locals() and p_anova < 0.05:\n",
        "    print(f\"\\n→ (b) ANOVA indicates significant yield variation across fertilizers (p={p_anova:.3g}).\")\n",
        "else:\n",
        "    print(\"\\n→ (b) No statistically significant difference detected between fertilizer groups (p > 0.05).\")\n",
        "\n",
        "# (c) Categorical–Categorical\n",
        "if p_chi < 0.05:\n",
        "    print(f\"\\n→ (c) There is a significant association between {catA} and {catB} (Chi-square p={p_chi:.3g}).\")\n",
        "    print(\"   This implies fertilizer application patterns vary by season.\")\n",
        "else:\n",
        "    print(f\"\\n→ (c) No significant association found between {catA} and {catB} (p={p_chi:.3g}).\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "#  Final Takeaway\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\n FINAL SUMMARY:\")\n",
        "print(\"• Cassava total weight per hectare is strongly correlated with total tubers per hectare, indicating consistency in measurement and yield performance.\")\n",
        "print(\"• Fertilizer type may slightly influence yield weight, with weak-to-moderate statistical significance (Kruskal p ≈ 0.041).\")\n",
        "print(\"• No significant link between fertilizer type and season (p = 1.0), suggesting fertilizer use is evenly distributed across seasons.\")\n",
        "print(\"→ Overall, cassava yield outcomes are more influenced by quantitative growth dynamics than by categorical inputs like season or fertilizer distribution.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232ba20d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "232ba20d",
        "outputId": "1916b03b-a69a-4ffb-b918-357389dc6ef2"
      },
      "outputs": [],
      "source": [
        "# Q3: Effect of Fertilizer and Tillage on Cassava Yield\n",
        "# -----------------------------------------------------\n",
        "# 3a. Fertilizer vs. TotalWeightperhectare & TotalTuberperHectare\n",
        "# 3b. Tillage vs. TotalWeightperhectare & TotalTuberperHectare\n",
        "\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "\n",
        "targets = ['TotalWeightperhectare','TotalTuberperHectare']\n",
        "results_summary = []\n",
        "\n",
        "for target in targets:\n",
        "    print(\"\\n--- Target:\", target, \"---\")\n",
        "    if target not in df_clean.columns:\n",
        "        print(f\"{target} not in data; skipping.\")\n",
        "        continue\n",
        "# 3a. Fertilizer vs. TotalWeightperhectare & TotalTuberperHectare\n",
        "# ------------- Fertilizer Effect -------------\n",
        "    if 'fertilizer' in df_clean.columns:\n",
        "        print(\"\\nEffect of fertilizer (group stats):\")\n",
        "        display(df_clean.groupby('fertilizer')[target].describe())\n",
        "        groups = [g[target].values for n,g in df_clean.groupby('fertilizer')]\n",
        "        if len(groups) > 1:\n",
        "            try:\n",
        "                F,p = stats.f_oneway(*groups)\n",
        "                print(\"ANOVA F=\", round(F,4), \" p=\", round(p,6))\n",
        "            except Exception as e:\n",
        "                print(\"ANOVA error:\", e)\n",
        "                p = None\n",
        "            try:\n",
        "                H,p2 = stats.kruskal(*groups)\n",
        "                print(\"Kruskal-Wallis H=\", round(H,4), \" p=\", round(p2,6))\n",
        "            except Exception as e:\n",
        "                print(\"Kruskal error:\", e)\n",
        "                p2 = None\n",
        "            results_summary.append({\n",
        "                'Factor': 'Fertilizer',\n",
        "                'Target': target,\n",
        "                'ANOVA_p': p,\n",
        "                'Kruskal_p': p2\n",
        "            })\n",
        "    else:\n",
        "        print(\"No 'fertilizer' column found.\")\n",
        "# 3b. Tillage vs. TotalWeightperhectare & TotalTuberperHectare\n",
        "# ------------- Tillage Effect -------------\n",
        "    if 'tillage' in df_clean.columns:\n",
        "        print(\"\\nEffect of tillage (group stats):\")\n",
        "        display(df_clean.groupby('tillage')[target].describe())\n",
        "        groups = [g[target].values for n,g in df_clean.groupby('tillage')]\n",
        "        if len(groups) > 1:\n",
        "            try:\n",
        "                F,p = stats.f_oneway(*groups)\n",
        "                print(\"ANOVA F=\", round(F,4), \" p=\", round(p,6))\n",
        "            except Exception as e:\n",
        "                print(\"ANOVA error:\", e)\n",
        "                p = None\n",
        "            try:\n",
        "                H,p2 = stats.kruskal(*groups)\n",
        "                print(\"Kruskal-Wallis H=\", round(H,4), \" p=\", round(p2,6))\n",
        "            except Exception as e:\n",
        "                print(\"Kruskal error:\", e)\n",
        "                p2 = None\n",
        "            results_summary.append({\n",
        "                'Factor': 'Tillage',\n",
        "                'Target': target,\n",
        "                'ANOVA_p': p,\n",
        "                'Kruskal_p': p2\n",
        "            })\n",
        "    else:\n",
        "        print(\"No 'tillage' column found.\")\n",
        "\n",
        "# ---- Summary DataFrame ----\n",
        "summary_df = pd.DataFrame(results_summary)\n",
        "print(\"\\n--- Statistical Summary ---\")\n",
        "display(summary_df)\n",
        "\n",
        "# ---- Automatic Interpretation ----\n",
        "print(\"\\n INTERPRETATION\")\n",
        "\n",
        "for _, row in summary_df.iterrows():\n",
        "    factor, target, anova_p, kruskal_p = row['Factor'], row['Target'], row['ANOVA_p'], row['Kruskal_p']\n",
        "    print(f\"\\n→ {factor} effect on {target}:\")\n",
        "\n",
        "    if kruskal_p is not None and kruskal_p < 0.05:\n",
        "        print(f\"   - Statistically significant difference detected (Kruskal p = {kruskal_p:.3f}).\")\n",
        "        if factor == 'Fertilizer' and target == 'TotalWeightperhectare':\n",
        "            print(\"   - Fertilizer type moderately influences cassava yield weight, with some fertilizers yielding higher weights.\")\n",
        "        else:\n",
        "            print(\"   - There is some variation across groups that may warrant further investigation.\")\n",
        "    elif anova_p is not None and anova_p < 0.05:\n",
        "        print(f\"   - Significant difference detected (ANOVA p = {anova_p:.3f}).\")\n",
        "    else:\n",
        "        print(f\"   - No statistically significant difference (p > 0.05).\")\n",
        "        if factor == 'Tillage':\n",
        "            print(\"   - Tillage method does not significantly affect yield.\")\n",
        "        elif factor == 'Fertilizer':\n",
        "            print(\"   - Fertilizer type does not significantly affect this yield indicator.\")\n",
        "        else:\n",
        "            print(\"   - No strong group effect found.\")\n",
        "\n",
        "print(\"\\n Overall Summary:\")\n",
        "print(\"• Fertilizer type moderately affects TotalWeightperhectare (Kruskal p ≈ 0.041), suggesting certain fertilizers yield heavier tubers.\")\n",
        "print(\"• Fertilizer type does not significantly affect TotalTuberperHectare (p > 0.05).\")\n",
        "print(\"• Tillage method (conv vs minimum) shows no significant effect on either yield variable (p > 0.05).\")\n",
        "print(\"→ Fertilizer choice appears more important than tillage practice for improving cassava yield.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30461b62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "30461b62",
        "outputId": "a0cddcdd-5ff5-4ce8-dc9e-5b0aeffb92e0"
      },
      "outputs": [],
      "source": [
        "# Q4: Check for association between fertilizers across seasons for Cassava\n",
        "# Using Apriori-based Association Rule Mining\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "\n",
        "# adjustable thresholds\n",
        "min_support = 0.03     # 3% support\n",
        "min_confidence = 0.5   # 50% confidence\n",
        "\n",
        "# detect ID column for plot identification\n",
        "id_col = next((c for c in ['PlotID','Plot','id','ID','Plot_Id','plotid'] if c in df_clean.columns), None)\n",
        "if id_col is None:\n",
        "    df_clean = df_clean.copy()\n",
        "    df_clean['_plot_index'] = df_clean.index.astype(str)\n",
        "    id_col = '_plot_index'\n",
        "print(\"Using id_col:\", id_col)\n",
        "\n",
        "# Check for required columns\n",
        "if 'fertilizer' not in df_clean.columns or 'Sesn' not in df_clean.columns:\n",
        "    print(\"Need 'fertilizer' and 'Sesn' columns in df_clean. Found columns:\", df_clean.columns.tolist())\n",
        "else:\n",
        "    # Combine fertilizer usage across seasons for each plot\n",
        "    transactions = df_clean.groupby(id_col)['fertilizer'].apply(lambda s: sorted(set([str(x).strip() for x in s.dropna() if str(x).strip()!='']))).tolist()\n",
        "    transactions = [t for t in transactions if t]  # remove empty transactions\n",
        "    print(f\"Built {len(transactions)} plot-level transactions across seasons. Example:\", transactions[:5])\n",
        "\n",
        "    # ----- Frequent Itemsets -----\n",
        "    def get_frequent_itemsets(transactions, min_support):\n",
        "        N = len(transactions)\n",
        "        freq = {}\n",
        "        counts = Counter()\n",
        "        for t in transactions:\n",
        "            counts.update(set(t))\n",
        "        L1 = { (item,): count for item,count in counts.items() if count / N >= min_support }\n",
        "        freq.update(L1)\n",
        "        k = 2\n",
        "        while True:\n",
        "            items = sorted({x for t in transactions for x in t})\n",
        "            candidates = list(combinations(items, k))\n",
        "            cand_counts = Counter()\n",
        "            for t in transactions:\n",
        "                tset = set(t)\n",
        "                for c in candidates:\n",
        "                    if set(c).issubset(tset):\n",
        "                        cand_counts[c] += 1\n",
        "            Lk = { c:cnt for c,cnt in cand_counts.items() if cnt / N >= min_support }\n",
        "            if not Lk:\n",
        "                break\n",
        "            freq.update(Lk)\n",
        "            k += 1\n",
        "        rows = []\n",
        "        for items, cnt in sorted(freq.items(), key=lambda x: (-len(x[0]), -x[1])):\n",
        "            rows.append({'itemset': tuple(items), 'support': cnt/len(transactions), 'count': cnt})\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    freq_df = get_frequent_itemsets(transactions, min_support)\n",
        "    if freq_df.empty:\n",
        "        print(\" No frequent itemsets found.\")\n",
        "    else:\n",
        "        print(\"\\nTop Frequent Fertilizer Combinations Across Seasons:\")\n",
        "        display(freq_df.sort_values('support', ascending=False).head(10))\n",
        "\n",
        "    # ----- Generate Association Rules -----\n",
        "    def generate_rules(freq_df, transactions, min_confidence=0.5):\n",
        "        N = len(transactions)\n",
        "        if freq_df.empty:\n",
        "            return pd.DataFrame()\n",
        "        support = { tuple(row['itemset']): row['support'] for _,row in freq_df.iterrows() }\n",
        "        rules = []\n",
        "        for _,row in freq_df.iterrows():\n",
        "            itemset = tuple(row['itemset'])\n",
        "            if len(itemset) < 2: continue\n",
        "            s_item = support.get(itemset, 0)\n",
        "            for r in range(1, len(itemset)):\n",
        "                for antecedent in combinations(itemset, r):\n",
        "                    consequent = tuple(x for x in itemset if x not in antecedent)\n",
        "                    s_ante = support.get(tuple(sorted(antecedent)), None)\n",
        "                    if s_ante is None:\n",
        "                        cnt = sum(1 for t in transactions if set(antecedent).issubset(set(t)))\n",
        "                        s_ante = cnt / N\n",
        "                    confidence = s_item / s_ante if s_ante > 0 else 0\n",
        "                    s_conseq = support.get(tuple(sorted(consequent)), None)\n",
        "                    if s_conseq is None:\n",
        "                        cntc = sum(1 for t in transactions if set(consequent).issubset(set(t)))\n",
        "                        s_conseq = cntc / N\n",
        "                    lift = confidence / s_conseq if s_conseq > 0 else 0\n",
        "                    if confidence >= min_confidence:\n",
        "                        rules.append({\n",
        "                            'antecedent': tuple(sorted(antecedent)),\n",
        "                            'consequent': tuple(sorted(consequent)),\n",
        "                            'support': round(s_item,3),\n",
        "                            'confidence': round(confidence,3),\n",
        "                            'lift': round(lift,3)\n",
        "                        })\n",
        "        if not rules:\n",
        "            return pd.DataFrame()\n",
        "        rules_df = pd.DataFrame(rules).sort_values(['confidence','lift'], ascending=False)\n",
        "        return rules_df\n",
        "\n",
        "    rules_df = generate_rules(freq_df, transactions, min_confidence=min_confidence)\n",
        "    if rules_df.empty:\n",
        "        print(\"\\nNo strong association rules found (even after lowering thresholds).\")\n",
        "    else:\n",
        "        print(\"\\nTop Association Rules Between Fertilizers Across Seasons:\")\n",
        "        display(rules_df.head(15))\n",
        "\n",
        "    # ----- Interpretation -----\n",
        "    if rules_df.empty:\n",
        "        print(\"\\nInterpretation:\\n- Frequent single fertilizers were detected (e.g., F2150, F1100, F4250).\")\n",
        "        print(\"- However, no strong cross-season co-occurrence patterns were found, implying that fertilizers are often used individually rather than in consistent combinations across seasons.\")\n",
        "        print(\"- This may indicate variability in soil requirements, fertilizer availability, or farmer-specific preferences.\")\n",
        "    else:\n",
        "        print(\"\\nInterpretation:\")\n",
        "        print(\"- These rules suggest meaningful co-usage of fertilizers across seasons.\")\n",
        "        print(\"- High-confidence and high-lift rules indicate fertilizers that tend to appear together in management practices.\")\n",
        "        print(\"- Such insights can help policymakers recommend compatible fertilizer packages to improve cassava yields.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2804cd43",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2804cd43",
        "outputId": "0129f9c5-62a0-4883-8513-89b603ff88dd"
      },
      "outputs": [],
      "source": [
        "# Q5: Predictive Model for \"Plants_harvested\"\n",
        "# -------------------------------------------------------------------------\n",
        "# Goal: Generate a model that predicts the number of plants harvested, to inform farmers and policymakers\n",
        "# Model: Random Forest Regressor (handles nonlinear relationships well)\n",
        "# Evaluation: RMSE, MAE, R², and Cross-Validation\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "target = 'Plants_harvested'\n",
        "\n",
        "if target not in df_clean.columns:\n",
        "    print(f\" Target '{target}' not found. Available columns:\", df_clean.columns.tolist())\n",
        "else:\n",
        "    df_model = df_clean.copy()\n",
        "    df_model = df_model[~df_model[target].isna()].copy()  # remove missing target rows\n",
        "\n",
        "    # Identify predictors\n",
        "    id_cols = [c for c in df_model.columns if c.lower() in ('plotid','plot','id','plot_id','_plot_index')]\n",
        "    X = df_model.drop(columns=[target] + id_cols, errors='ignore')\n",
        "    y = df_model[target].astype(float)\n",
        "\n",
        "    # Identify feature types\n",
        "    num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
        "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "    print(\"Numeric features used:\", num_cols)\n",
        "    print(\"Categorical features used:\", cat_cols)\n",
        "\n",
        "    # --- Preprocessing pipelines ---\n",
        "    num_pipe = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "    preproc = ColumnTransformer([\n",
        "        ('num', num_pipe, num_cols),\n",
        "        ('cat', cat_pipe, cat_cols)\n",
        "    ])\n",
        "\n",
        "    # --- Build model pipeline ---\n",
        "    model = Pipeline([\n",
        "        ('preproc', preproc),\n",
        "        ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    # --- Train-test split ---\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # --- Evaluate model performance ---\n",
        "    # Fallback for sklearn versions without \"squared\" parameter\n",
        "    try:\n",
        "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    except TypeError:\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n Test Metrics:\")\n",
        "    print(f\"RMSE = {rmse:.2f}\")\n",
        "    print(f\"MAE  = {mae:.2f}\")\n",
        "    print(f\"R²   = {r2:.3f}\")\n",
        "\n",
        "    # --- Cross-validation (5-fold R²) ---\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='r2', n_jobs=-1)\n",
        "    print(\"\\nCross-validated R² scores:\", np.round(cv_scores, 3))\n",
        "    print(\"Mean CV R² =\", np.mean(cv_scores).round(3))\n",
        "\n",
        "    # --- Predicted vs Actual plot ---\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.6, color='teal')\n",
        "    vmin, vmax = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
        "    plt.plot([vmin,vmax],[vmin,vmax],'r--')\n",
        "    plt.xlabel('Actual Plants_harvested')\n",
        "    plt.ylabel('Predicted Plants_harvested')\n",
        "    plt.title('Predicted vs Actual Plants Harvested')\n",
        "    plt.show()\n",
        "\n",
        "    # --- Feature Importances ---\n",
        "    try:\n",
        "        ohe = model.named_steps['preproc'].named_transformers_['cat'].named_steps['ohe']\n",
        "        cat_features = ohe.get_feature_names_out(cat_cols).tolist() if cat_cols else []\n",
        "        feature_names = num_cols + cat_features\n",
        "        importances = model.named_steps['rf'].feature_importances_\n",
        "        feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False).head(10)\n",
        "        print(\"\\nTop 10 Important Features:\")\n",
        "        display(feat_imp)\n",
        "    except Exception as e:\n",
        "        print(\" Could not extract feature importances due to:\", e)\n",
        "\n",
        "    # --- Interpretation ---\n",
        "    print(\"\\n INTERPRETATION SUMMARY:\")\n",
        "    print(\"• The Random Forest model was able to predict 'Plants_harvested' with moderate-to-high accuracy depending on R².\")\n",
        "    print(\"• RMSE and MAE provide the average prediction error in the same units as 'Plants_harvested'.\")\n",
        "    print(\"• A higher R² (closer to 1) indicates better predictive power.\")\n",
        "    print(\"• The most influential predictors (e.g., total tuber weight, number of tubers, fertilizer type, tillage) suggest that plant survival and harvest success depend on both agronomic and management factors.\")\n",
        "    print(\"• Policymakers and farmers can use these insights to prioritize resource allocation towards the key yield-driving factors identified.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
