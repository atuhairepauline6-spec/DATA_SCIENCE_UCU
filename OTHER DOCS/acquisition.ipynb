{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc11afa",
   "metadata": {},
   "source": [
    "\n",
    "# acquisition.ipynb — Data Acquisition (Web Scraping / API)\n",
    "\n",
    "This notebook collects **global university ranking data** (name, rank, country, score, year) from a public,\n",
    "multi-page ranking website (default: **CWUR**). It demonstrates:\n",
    "\n",
    "- Robust HTTP requests with retry/backoff\n",
    "- Polite scraping (User-Agent, small delay)\n",
    "- Pagination across **at least 5 pages**\n",
    "- Output as a single **CSV** and a **list of dicts** (saved to `.jsonl`)\n",
    "\n",
    "> ⚠️ Run-time note: Some ranking sites change structure or rate-limit. If the default source fails, switch to the backup\n",
    "  source (QS/THE/CWTS Leiden) by editing `SOURCE` and CSS selectors in the `parse_page()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d89dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard imports\n",
    "from __future__ import annotations\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# I/O paths\n",
    "OUT_DIR = Path(\"./data_raw\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_CSV = OUT_DIR / \"rankings_raw.csv\"\n",
    "RAW_JSONL = OUT_DIR / \"rankings_raw.jsonl\"\n",
    "\n",
    "# Scrape configuration\n",
    "@dataclass\n",
    "class SourceConfig:\n",
    "    name: str\n",
    "    base_url: str\n",
    "    pagination_param: str  # e.g., 'start' or 'page'\n",
    "    first_page: int\n",
    "    per_page: int\n",
    "    pages: int            # scrape at least 5 pages\n",
    "    year: int\n",
    "    headers: Dict[str, str]\n",
    "\n",
    "# --- Default: CWUR (example) ---\n",
    "# CWUR pagination often uses a \"start\" offset (0, 100, 200, ...). Adjust if needed.\n",
    "SOURCE = SourceConfig(\n",
    "    name=\"CWUR\",\n",
    "    base_url=\"https://cwur.org/{year}.php\",\n",
    "    pagination_param=\"start\",\n",
    "    first_page=0,\n",
    "    per_page=100,     # expected per-page rows\n",
    "    pages=6,          # 6 pages => ~600 rows (>= 500 requirement)\n",
    "    year=2024,\n",
    "    headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/120.0 Safari/537.36\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_session() -> requests.Session:\n",
    "    \"\"\"Create a session with retry/backoff for resilience.\"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    session.headers.update(SOURCE.headers)\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d8cc90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     offset \u001b[38;5;241m=\u001b[39m SOURCE\u001b[38;5;241m.\u001b[39mfirst_page \u001b[38;5;241m+\u001b[39m page_index \u001b[38;5;241m*\u001b[39m SOURCE\u001b[38;5;241m.\u001b[39mper_page\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSOURCE\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;241m.\u001b[39mformat(year\u001b[38;5;241m=\u001b[39mSOURCE\u001b[38;5;241m.\u001b[39myear)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSOURCE\u001b[38;5;241m.\u001b[39mpagination_param\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_page\u001b[39m(html: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict]:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a single ranking page and return rows as dicts.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Expected columns: Rank, University, Country, Score (Overall or similar).\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Adjust CSS selectors to match the site's structure.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def page_url(page_index: int) -> str:\n",
    "    \"\"\"Build a paginated URL for the given page index.\"\"\"\n",
    "    # Example CWUR style: https://cwur.org/2024.php?start=0,100,200,...\n",
    "    offset = SOURCE.first_page + page_index * SOURCE.per_page\n",
    "    return f\"{SOURCE.base_url.format(year=SOURCE.year)}?{SOURCE.pagination_param}={offset}\"\n",
    "\n",
    "def parse_page(html: str) -> List[Dict]:\n",
    "    \"\"\"Parse a single ranking page and return rows as dicts.\n",
    "\n",
    "    Expected columns: Rank, University, Country, Score (Overall or similar).\n",
    "    Adjust CSS selectors to match the site's structure.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        # Some sites use specific class names; you can refine as needed.\n",
    "        table = soup.find(\"table\", {\"class\": \"table\"}) or soup.find(\"tbody\")\n",
    "        if not table:\n",
    "            return []\n",
    "\n",
    "    rows = []\n",
    "    # Heuristic: iterate over table rows; extract cells\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cells = [c.get_text(strip=True) for c in tr.find_all([\"td\", \"th\"])]\n",
    "        # Try to infer columns by typical positions (Rank, University, Country, Score)\n",
    "        if len(cells) < 3:\n",
    "            continue\n",
    "\n",
    "        # Attempt flexible mapping\n",
    "        rank = None\n",
    "        name = None\n",
    "        country = None\n",
    "        score = None\n",
    "\n",
    "        # Common CWUR layout: [World Rank, University, Country, Score, ...]\n",
    "        try:\n",
    "            rank = cells[0] or None\n",
    "            name = cells[1] or None\n",
    "            country = cells[2] or None\n",
    "            # Score may be at 3rd or 4th index depending on site\n",
    "            score = cells[3] if len(cells) > 3 else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Skip header rows or incomplete ones\n",
    "        if (rank and rank.lower() == \"world rank\") or (name and name.lower() == \"university\"):\n",
    "            continue\n",
    "        if not name or not rank:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"Year\": SOURCE.year,\n",
    "            \"Rank\": rank,\n",
    "            \"University\": name,\n",
    "            \"Country\": country,\n",
    "            \"Overall_Score\": score\n",
    "        })\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape() -> List[Dict]:\n",
    "    session = make_session()\n",
    "    all_rows: List[Dict] = []\n",
    "    for p in range(SOURCE.pages):\n",
    "        url = page_url(p)\n",
    "        print(f\"Fetching page {p+1}/{SOURCE.pages}: {url}\")\n",
    "        resp = session.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        page_rows = parse_page(resp.text)\n",
    "        print(f\"  Found {len(page_rows)} rows.\")\n",
    "        all_rows.extend(page_rows)\n",
    "        time.sleep(0.7)  # Polite delay\n",
    "    print(f\"Total rows scraped: {len(all_rows)}\")\n",
    "    return all_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_jsonl(rows: List[Dict], path: Path) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def save_csv(rows: List[Dict], path: Path) -> None:\n",
    "    if not rows:\n",
    "        return\n",
    "    keys = list(rows[0].keys())\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=keys)\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rows = scrape()\n",
    "    save_jsonl(rows, RAW_JSONL)\n",
    "    save_csv(rows, RAW_CSV)\n",
    "    print(f\"Saved: {RAW_CSV} and {RAW_JSONL}\")\n",
    "    # Also show a small preview\n",
    "    for r in rows[:5]:\n",
    "        print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
